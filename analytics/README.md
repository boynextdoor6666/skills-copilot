# Analytics Stack: Kafka + PySpark + ClickHouse

–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫ CineVibe –¥–ª—è —Å–±–æ—Ä–∞, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±—ã—Ç–∏–π –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏.

## üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   NestJS    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ    Kafka    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   PySpark   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ ClickHouse  ‚îÇ
‚îÇ   Backend   ‚îÇ     ‚îÇ   (Events)  ‚îÇ     ‚îÇ  (ETL/Agg)  ‚îÇ     ‚îÇ   (OLAP)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                           ‚îÇ                                       ‚îÇ
                           ‚îÇ                                       ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ  Kafka UI   ‚îÇ                         ‚îÇ   NestJS    ‚îÇ
                    ‚îÇ  (Monitor)  ‚îÇ                         ‚îÇ  Analytics  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

```bash
cd analytics
docker-compose up -d
```

–≠—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç:
- **Zookeeper**: –ø–æ—Ä—Ç 2181
- **Kafka**: –ø–æ—Ä—Ç 9092 (–≤–Ω–µ—à–Ω–∏–π), 29092 (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π)
- **Kafka UI**: http://localhost:8090
- **ClickHouse**: –ø–æ—Ä—Ç 8123 (HTTP), 9000 (Native)
- **Spark Master**: http://localhost:8081
- **Spark Worker**: –ø–æ–¥–∫–ª—é—á–µ–Ω –∫ Master

### 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å Kafka
docker exec cinevibe-kafka kafka-topics.sh --bootstrap-server localhost:9092 --list

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å ClickHouse
docker exec cinevibe-clickhouse clickhouse-client --query "SHOW DATABASES"

# –û—Ç–∫—Ä—ã—Ç—å Kafka UI
# http://localhost:8090
```

### 3. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Backend

–î–æ–±–∞–≤—å—Ç–µ –≤ `.env`:

```env
# –í–∫–ª—é—á–∏—Ç—å Kafka
KAFKA_ENABLED=true
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=cinevibe-backend

# –í–∫–ª—é—á–∏—Ç—å ClickHouse
CLICKHOUSE_ENABLED=true
CLICKHOUSE_HOST=localhost
CLICKHOUSE_PORT=8123
CLICKHOUSE_DATABASE=analytics
```

### 4. –ó–∞–ø—É—Å–∫ PySpark Job

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
pip install -r pyspark/requirements.txt

# –ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ —Å—Ç—Ä–∏–º—ã
python pyspark/analytics_jobs.py --job stream-all

# –ò–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∏–º—ã
python pyspark/analytics_jobs.py --job stream-reviews
python pyspark/analytics_jobs.py --job stream-users
python pyspark/analytics_jobs.py --job stream-content

# Batch –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
python pyspark/analytics_jobs.py --job batch-all --date 2025-11-29
```

## üìä Kafka Topics

| Topic | –û–ø–∏—Å–∞–Ω–∏–µ | –°–æ–±—ã—Ç–∏—è |
|-------|----------|---------|
| `reviews` | –°–æ–±—ã—Ç–∏—è –æ—Ç–∑—ã–≤–æ–≤ | `review_created`, `review_deleted`, `rating_changed` |
| `users` | –°–æ–±—ã—Ç–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π | `user_registered`, `user_login`, `achievement_unlocked` |
| `content` | –°–æ–±—ã—Ç–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞ | `content_viewed`, `content_searched`, `content_imported` |

## üóÉÔ∏è ClickHouse Tables

### Raw Events
- `analytics.reviews_events` - —Å—ã—Ä—ã–µ —Å–æ–±—ã—Ç–∏—è –æ—Ç–∑—ã–≤–æ–≤
- `analytics.user_events` - —Å–æ–±—ã—Ç–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- `analytics.content_events` - —Å–æ–±—ã—Ç–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞

### Aggregations
- `analytics.content_daily_stats` - –¥–Ω–µ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
- `analytics.hourly_activity` - –ø–æ—á–∞—Å–æ–≤–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
- `analytics.user_activity_daily` - –¥–Ω–µ–≤–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- `analytics.content_popularity` - –ø–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞

### Materialized Views
- `analytics.mv_content_review_counts` - —Å—á—ë—Ç—á–∏–∫–∏ –æ—Ç–∑—ã–≤–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- `analytics.mv_hourly_events` - –ø–æ—á–∞—Å–æ–≤—ã–µ —Å—á—ë—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π

## üîå API Endpoints

### –°—Ç–∞—Ç—É—Å
```
GET /api/analytics/realtime/status
```

### –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
```
GET /api/analytics/realtime/content/:contentId
GET /api/analytics/realtime/top-content?type=MOVIE&limit=10&days=7
```

### –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
```
GET /api/analytics/realtime/user/:userId
```

### –¢—Ä–µ–Ω–¥—ã
```
GET /api/analytics/realtime/trends/reviews?days=30
GET /api/analytics/realtime/distribution/ratings?type=MOVIE
GET /api/analytics/realtime/emotions?contentId=1
GET /api/analytics/realtime/activity/hourly
```

## üìà –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ ClickHouse

### –¢–æ–ø –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∑–∞ –Ω–µ–¥–µ–ª—é
```sql
SELECT 
    content_id,
    content_type,
    sum(views_count) as views,
    sum(reviews_count) as reviews,
    avg(avg_rating) as rating
FROM analytics.content_daily_stats
WHERE date >= today() - 7
GROUP BY content_id, content_type
ORDER BY views + reviews * 10 DESC
LIMIT 10;
```

### –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —á–∞—Å–∞–º
```sql
SELECT 
    toHour(hour) as h,
    sum(count) as total
FROM analytics.hourly_activity
WHERE hour >= now() - INTERVAL 24 HOUR
GROUP BY h
ORDER BY h;
```

### –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
```sql
SELECT 
    floor(rating) as rating_bucket,
    count() as cnt
FROM analytics.reviews_events
WHERE rating IS NOT NULL
GROUP BY rating_bucket
ORDER BY rating_bucket;
```

## üõ†Ô∏è –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞

### –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∞–π–ª–æ–≤

```
analytics/
‚îú‚îÄ‚îÄ docker-compose.yml          # –ò–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞
‚îú‚îÄ‚îÄ README.md                   # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
‚îú‚îÄ‚îÄ clickhouse/
‚îÇ   ‚îî‚îÄ‚îÄ init/
‚îÇ       ‚îî‚îÄ‚îÄ 01_init_schema.sql  # –°—Ö–µ–º–∞ ClickHouse
‚îî‚îÄ‚îÄ pyspark/
    ‚îú‚îÄ‚îÄ requirements.txt        # Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
    ‚îú‚îÄ‚îÄ stream_to_clickhouse.py # –ë–∞–∑–æ–≤—ã–π —Å—Ç—Ä–∏–º (legacy)
    ‚îî‚îÄ‚îÄ analytics_jobs.py       # –í—Å–µ PySpark jobs
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Å–æ–±—ã—Ç–∏—è

1. –î–æ–±–∞–≤—å—Ç–µ —Ç–∏–ø –≤ `KafkaService` (NestJS)
2. –î–æ–±–∞–≤—å—Ç–µ —Å—Ö–µ–º—É –≤ `analytics_jobs.py` (PySpark)
3. –î–æ–±–∞–≤—å—Ç–µ —Ç–∞–±–ª–∏—Ü—É –≤ `01_init_schema.sql` (ClickHouse)
4. –î–æ–±–∞–≤—å—Ç–µ endpoint –≤ `AnalyticsController` (NestJS)

## ‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### Kafka
| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | Default | –û–ø–∏—Å–∞–Ω–∏–µ |
|------------|---------|----------|
| `KAFKA_ENABLED` | `false` | –í–∫–ª—é—á–∏—Ç—å Kafka |
| `KAFKA_BROKERS` | `localhost:9092` | –ê–¥—Ä–µ—Å–∞ –±—Ä–æ–∫–µ—Ä–æ–≤ |
| `KAFKA_CLIENT_ID` | `cinevibe-backend` | ID –∫–ª–∏–µ–Ω—Ç–∞ |

### ClickHouse
| –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è | Default | –û–ø–∏—Å–∞–Ω–∏–µ |
|------------|---------|----------|
| `CLICKHOUSE_ENABLED` | `false` | –í–∫–ª—é—á–∏—Ç—å ClickHouse |
| `CLICKHOUSE_HOST` | `localhost` | –•–æ—Å—Ç |
| `CLICKHOUSE_PORT` | `8123` | HTTP –ø–æ—Ä—Ç |
| `CLICKHOUSE_DATABASE` | `analytics` | –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö |

## üîß Troubleshooting

### Kafka –Ω–µ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs cinevibe-kafka

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ Zookeeper —Ä–∞–±–æ—Ç–∞–µ—Ç
docker exec cinevibe-zookeeper zkServer.sh status
```

### ClickHouse –æ—à–∏–±–∫–∏
```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏
docker logs cinevibe-clickhouse

# –ü–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ CLI
docker exec -it cinevibe-clickhouse clickhouse-client
```

### PySpark –Ω–µ –≤–∏–¥–∏—Ç Kafka
```bash
# –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π bootstrap server
# –î–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏: localhost:9092
# –í–Ω—É—Ç—Ä–∏ Docker: kafka:29092
```
